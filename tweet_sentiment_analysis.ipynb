{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1KUWbEtuHm518YUILaJbkvAye5iZG8_Jt",
      "authorship_tag": "ABX9TyMwHkzd1Ppjc1HoBM+DmE9H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnaudstdr/tweet_sentiment_analysis/blob/main/tweet_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANALYSE DES SENTIMENTS SUR TWITTER\n",
        "Ce projet permet d'analyser les sentiments exprimés dans des tweets enutilisantdes techniques de traitement du langage naturel (NLP) et de deep learning.\n",
        "Adapté pour un dataset Kaggle avec des fichiers d'entraînement et de validation séparés.\n",
        "\n",
        "## Sommaire\n",
        "- <a href=\"#importation\">1. Importation des bibliothèques</a>\n",
        "- <a href=\"#ressources-nltk\">2. Téléchargment des ressources NLTK</a>\n",
        "- <a href=\"#charg-explo-donnees\">3. Chargement et Exploration des données</a>\n",
        "- <a href=\"#pretraitement\">4. Prétraitement du texte</a>\n",
        "- <a href=\"#visualisation\">5. Visualisation des données</a>\n",
        "- <a href=\"#tradi-tdidf\">6. Approche traditionnelle avec TF-IDF</a>"
      ],
      "metadata": {
        "id": "ZdTnwyIA2xfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. <a id=\"importation\">Importation des bibliothèques</a>"
      ],
      "metadata": {
        "id": "4JKoXDDp3k1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lF6vtBZ_5WBh",
        "outputId": "cda97b62-2073-4022-b29c-c31a7a8700bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.19.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.2 (from gradio)\n",
            "  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.9.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.19.0-py3-none-any.whl (62.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.9 ffmpy-0.5.0 gradio-5.19.0 gradio-client-1.7.2 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.8 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.45.3 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g3SecJTj177b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "# import gradio as gr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. <a id=\"ressources-nltk\">Téléchargement des ressources NLTK</a>\n",
        "L'importation des bibliothèques seules ne suffit pas pour que `nltk` fonctionne correctement. Les ressources comme les stopwords, le tokeniseur et le lemmatiseur ne sont pas incluses par défaut. Elles doivent être téléchargées séparément.\n",
        "\n"
      ],
      "metadata": {
        "id": "E_h5go_Q7Yrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pz-fL9g46Gz",
        "outputId": "535ebc07-4f3b-459f-c3d3-9e87c167c29f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. <a id=\"charg-explo-donnees\">Chargment et Exploration des données</a>"
      ],
      "metadata": {
        "id": "ZgM3INgE-G4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(train_path, val_path=None):\n",
        "  # Chargement du dataset d'entraînement\n",
        "  train_data = pd.read_csv(train_path)\n",
        "  print(f\"Diemnsion du dataset d'entraînement : {train_data.shape}\")\n",
        "  print(\"Aperçu du dataset d'entraînement : \")\n",
        "  print(train_data.head())\n",
        "\n",
        "  # Vérification des colonnes du dataset\n",
        "  print(\"\\nColonnes du dataset d'entraînement : \")\n",
        "  print(train_data.columns.tolist())\n",
        "\n",
        "  # si un fichier de validation\n",
        "  if val_path:\n",
        "    val_data = pd.read_csv(val_path)\n",
        "    print(\"\\nDimensions du dataset de validation : {val_data.shape}\")\n",
        "    print(val_data.head())\n",
        "    return train_data, val_data\n",
        "  else:\n",
        "    return train_data, None"
      ],
      "metadata": {
        "id": "leNhUnk883OM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. <a id=\"pretraitement\">Prétraitement du texte</a>\n",
        "Pourquoi le prétraitement est important ?\n",
        "1. **Réduction du bruit** : Les tweets contiennent souvent beucoup d'éléments non pertinents pour l'analyse des sentiments (URLs, mentions, etc.).\n",
        "2. **Normalisation** : Les différentes formes d'un même mot (pluriels, conjugaisons) sont ramenées à une forme santard.\n",
        "3. **Réduction de la dimensionnalité** : En supprimant les stopwords et en utilisant la lemmatisation, on réduit le nombre de mots uniques, ce qui facilite l'apprentissage des modèles.\n",
        "4. **Amélioration des performances** : Un texte bien prétraité permet aux modèles de se concentrer sur les mots et expressions qui véhiculent réeelement un sentiment.  "
      ],
      "metadata": {
        "id": "Eo99ZhvoG_pW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "  # Vérifier si le texte est une chaîne de caractères\n",
        "  if not isinstance(text, str):\n",
        "    return \"\"\n",
        "\n",
        "  # Convertir en minuscules\n",
        "  text = text.lower()\n",
        "\n",
        "  # Supprimer les URLs\n",
        "  text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "  # Supprimer les hashtags\n",
        "  text = re.sub(r'#\\w+', '', text)\n",
        "\n",
        "  # Supprimer les caractères non-alphanumériques\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "  # Supprimer les chiffres\n",
        "  text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "  # Tokenisation\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # Supression des stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "  # Lemmatisation\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "  # Rejoindre les tokens\n",
        "  return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "yonVcc-7HMqc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Applique le prétraitement à l'ensemble du dataste\"\"\"\n",
        "def prepare_datdset(data, text_column, label_column):\n",
        "  # Vérifier que les colonnes existent\n",
        "  if text_column not in data.columns:\n",
        "    raise ValueError(f\"Les colonnes de texte '{text_column}' n'existe pas dans le dataset\")\n",
        "  if label_column not in data.columns:\n",
        "    raise ValueError(f\"La colonne d'étiquette '{label_column}' n'existe pas dans le datatset\")\n",
        "\n",
        "  # Appliquer le prétraitement au text\n",
        "  data['clean_text'] = data[label_column].apply(preprocess_text)\n",
        "\n",
        "  # Identifier les valeurs uniques dans la colonne des sentiments\n",
        "  unique_sentiments = data[label_column].unique()\n",
        "  print(f\"Valeurs uniques de sentiment trouvées : {unique_sentiments}\")\n",
        "\n",
        "  # Créer un mapping des sentiments basé sur les valeurs trouvées\n",
        "  # Partie pouvant nécessitant un ajustement en fonction du format du dataset\n",
        "  sentiment_map = {}\n",
        "\n",
        "  # Essayer de détecter automatiquement le format du sentiment\n",
        "  if set(unique_sentiments).issubset({0, 1}) or set(unique_sentiments).issubset({'0', '1'}):\n",
        "    # Dataset binaire (positif/négatif)\n",
        "    sentiment_map = {0: 0, 1: 1, '0': 0, '1': 1}\n",
        "    print(\"Format détecté : Bianire (négatif/positif)\")\n",
        "  elif set(unique_sentiments).issubset({-1, 0, 1}) or set(unique_sentiments).issubset({'-1', '0', '1'}):\n",
        "    # Dataset ternaire avec -1, 0, 1\n",
        "    sentiment_map = {-1: 0, 0: 2, 1:1, '-1':0, '0':2, '1':1}\n",
        "    print(\"Format détecté : Ternaire (-1=négatif, 0=neutre, 1=positif)\")\n",
        "  elif any(isinstance(x, str) and x.lower() in ['positive', 'negative', 'neutrel'] for x in unique_sentiments):\n",
        "    # Dataset avec texte\n",
        "    sentiment_map = {'negative': 0, 'neutral': 2, 'positive': 1}\n",
        "    print(\"Format détecté : Textuel (negative/neutral/positice)\")\n",
        "  else:\n",
        "    # Format non reconnu, créer un mapping générique\n",
        "    sentiment_map = {val: idx for idx, val in enumerate(unique_sentiments)}\n",
        "    print(f\"Format non reconnu. Mapping créé : {sentiments}\")\n",
        "\n",
        "  # Appliquer le mapping\n",
        "  data['sentiment_label'] = data[label_column].map(sentiment_map)\n",
        "\n",
        "  # Vérifier qu'il n'y pas de NaN dans les labels après le mapping\n",
        "  if data['sentiment_label'].isna().any():\n",
        "    print(\"ATTENTION : Certaines valeurs de sentiment n'ont pas pu être converties !\")\n",
        "    print(\"Valeurs problématiques : \", data[data['sentiment_label'].isna()][label_column].unique())\n",
        "    # Remplir les NaN avec une valeur par défaut (par exemple, 0 pour négatif)\n",
        "    data['sentiment_label'] = data['sentiment_label'].fillna(0)\n",
        "\n",
        "  return data\n"
      ],
      "metadata": {
        "id": "gYEfDHNXJmYl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. <a id=\"visualisation\">Visualisation des données</a>\n",
        "\n",
        "- La partie visualisation des données est conçue pour explorer et comprendre les caractéristiques de dataset avant de passer à la modélisation.\n",
        "- La fonction `visualize_data()` crée trois visaulisations principales pour analyser la distribution et les caractéristiques des sentiments dans les tweets."
      ],
      "metadata": {
        "id": "kmj2lXTYOpDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_data(data, sentiment_column, text_column):\n",
        "  \"\"\"Crée des visualisations pour explorer le dataset\"\"\"\n",
        "  # Distribution des sentiments\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  sentiment_counts = data[sentiment_column].value_counts()\n",
        "\n",
        "  #Création du palette de couleurs plus attrayante\n",
        "  colors = ['#ff9999', '#66b3ff', '#99ff99'][:len(sentiment_counts)]\n",
        "\n",
        "  # Affichage du graphique\n",
        "  ax = sentiment_counts.plot(kind='bar', color=colors)\n",
        "  for i, v in enumerate(sentiment_counts):\n",
        "    ax.text(i, v + 0.1, str(v), ha='center')\n",
        "\n",
        "  plt.title('Distribution des Sentiments', fontsize=14)\n",
        "  plt.xlabel('Sentiment', fontsize=12)\n",
        "  plt.ylabel('Nombre de Tweets', fontsize=12)\n",
        "  plt.tight_layout()\n",
        "  plt.savefig('sentiment_distribution.png')\n",
        "  print(\"Graphique de distributiondes sentiments sauvegardé dans 'sentiment_distribution.png\")\n",
        "\n",
        "  # Longueur des tweets par sentiment\n",
        "  data['text_length'] = data[text_column].astype(str).apply(len)\n",
        "\n",
        "  plt.figure(figsize=(12, 7))\n",
        "\n",
        "  # Utilisation de box plot avec swarmplot pour une meilleure visualisation\n",
        "  ax = sns.boxplot(x=sentiment_column, y='text_legnth', data=data, palette='Set2')\n",
        "  sns.swarmplot(x=sentiment_column, y='text_length', data=data, color='0.25', size=4, alpha=0.5)\n",
        "\n",
        "  plt.title('Longueyr des Tweets par Sentiment', fontsize=14)\n",
        "  plt.xlabel('Senriment', fontsize=12)\n",
        "  plt.ylabel('Longueur du Tweet (caractères)', fontsize=12)\n",
        "  plt.tight_layout()\n",
        "  plt.savefig('tweet_length_by_sentiment.png')\n",
        "  print(\"Graphique de longueur des tweets sauvegardé dans 'tweet_by_length_sentiment.png'\")\n",
        "\n",
        "  # Analyse des mots les plus fréquents par sentiment\n",
        "  from collections import Counter\n",
        "  import matplotlib.cm as cm\n",
        "\n",
        "  # Créer un DataFrame pour les mots les plus fréquents par sentiment\n",
        "  plt.figure(figsize=(15, 12))\n",
        "\n",
        "  # Définir le nombre de sentiments dans le dataset\n",
        "  num_sentiments = data['sentiment_label'].nunique()\n",
        "\n",
        "  # Ajuster le nombre de sous-graphique en fonction du nombre de sentiment\n",
        "  fig, axes = plt.subplots(1, num_sentiments, figsize=(15, 6))\n",
        "  if num_sentiments == 1:\n",
        "    axes = [axes]     # Assure que axes est toujours une liste\n",
        "\n",
        "  sentiment_names = {0: 'Négatif', 1: 'Positif', 2: 'Neutre'}\n",
        "\n",
        "  # Pour chaque sentiment, trouver les mots les plus fréquents\n",
        "  for i, sentiment_value in enumerate(sorted(data['sentiment_label'].unique())):\n",
        "    # Filtrer les tweets par sentiment\n",
        "    sentiment_data = data[data['sentiment_label'] == sentiment_value]\n",
        "\n",
        "    # Joindre tous les textes nettoyés\n",
        "    all_words = ' '.join(sentiment_data['clean_text'].astype(str)).split()\n",
        "\n",
        "    # Compter les mots\n",
        "    word_counts = Counter(all_words)\n",
        "\n",
        "    # Prendre les 15 mots les plus fréquents\n",
        "    most_common = word_counts.most_common(15)\n",
        "\n",
        "    # Créer des listes pour les graphiques\n",
        "    words = [word for word, count in most_common]\n",
        "    counts = [count for word, count in most_common]\n",
        "\n",
        "    # Tracer le graphique à barre horizontales\n",
        "    sentiment_name = sentiment_names.get(sentiment_value, f\"Sentiment {sentiment_value}\")\n",
        "    axes[i].barh(words, counts, color=cm.Set3(i / num_sentiments))\n",
        "    axes[i].set_title(f'Mots fréquent - {sentiment_name}')\n",
        "    axes[i].set_xlabel('Fréquence')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.savefig('frequent_words_by_sentiment.png')\n",
        "  print(\"Graphique des mots fréquents sauvegardé dans 'frequent_words_by_sentiment.png'\")\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "wRnMYAQOOlPS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ces visulisations constituent une étape d'analyse exploratoire des données (EDA) importante qui aide à :\n",
        "- Comprendre le déséquilibre éventuel des classes.\n",
        "- Voir si la longueur des tweets est corrélée au sentiment.\n",
        "- Identifier les mots caractéristiques de chaque sentiment."
      ],
      "metadata": {
        "id": "KSvk1Q7-b0eO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. <a id=\"tradi-tdidf\">Approche Traditionnelle avec TF-IDF</a>\n",
        "\n",
        "Cette section implémente une méthode classqieu d'analyse de sentiments basé sur des techniques de NLP plus traditionnelles, avant l'ère des transformers."
      ],
      "metadata": {
        "id": "xG0BjmCeZDZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_tfidf_model(train_data, val_data=None):\n",
        ""
      ],
      "metadata": {
        "id": "xZDHZjp4ZBrr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}