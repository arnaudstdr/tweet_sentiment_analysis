{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdTnwyIA2xfk"
   },
   "source": [
    "# ANALYSE DES SENTIMENTS SUR TWITTER\n",
    "Ce projet permet d'analyser les sentiments exprimés dans des tweets enutilisantdes techniques de traitement du langage naturel (NLP) et de deep learning.\n",
    "Adapté pour un dataset Kaggle avec des fichiers d'entraînement et de validation séparés.\n",
    "\n",
    "## Sommaire\n",
    "- <a href=\"#importation\">1. Importation des bibliothèques</a>\n",
    "- <a href=\"#ressources-nltk\">2. Téléchargment des ressources NLTK</a>\n",
    "- <a href=\"#charg-explo-donnees\">3. Chargement et Exploration des données</a>\n",
    "- <a href=\"#pretraitement\">4. Prétraitement du texte</a>\n",
    "- <a href=\"#visualisation\">5. Visualisation des données</a>\n",
    "- <a href=\"#tradi-tdidf\">6. Approche traditionnelle avec TF-IDF</a>\n",
    "- <a href=\"#deep-learning\">7. Approche Deep Learning avec Transformers</a>\n",
    "- <a href=\"#fonction-principale\">Foction Principale</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JKoXDDp3k1d"
   },
   "source": [
    "## 1. <a id=\"importation\">Importation des bibliothèques</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 24462,
     "status": "ok",
     "timestamp": 1741187866044,
     "user": {
      "displayName": "Arnaud Stadler",
      "userId": "13776147233644915596"
     },
     "user_tz": -60
    },
    "id": "g3SecJTj177b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_h5go_Q7Yrj"
   },
   "source": [
    "## 2. <a id=\"ressources-nltk\">Téléchargement des ressources NLTK</a>\n",
    "L'importation des bibliothèques seules ne suffit pas pour que `nltk` fonctionne correctement. Les ressources comme les stopwords, le tokeniseur et le lemmatiseur ne sont pas incluses par défaut. Elles doivent être téléchargées séparément.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 588,
     "status": "ok",
     "timestamp": 1741187871205,
     "user": {
      "displayName": "Arnaud Stadler",
      "userId": "13776147233644915596"
     },
     "user_tz": -60
    },
    "id": "9pz-fL9g46Gz",
    "outputId": "bb42eab5-adcb-411c-ae8c-43d47e24ae27"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgM3INgE-G4k"
   },
   "source": [
    "## 3. <a id=\"charg-explo-donnees\">Chargment et Exploration des données</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 703,
     "status": "ok",
     "timestamp": 1741188666887,
     "user": {
      "displayName": "Arnaud Stadler",
      "userId": "13776147233644915596"
     },
     "user_tz": -60
    },
    "id": "leNhUnk883OM",
    "outputId": "e8b875c8-62e4-4250-d642-ae0d665ea1a5"
   },
   "outputs": [],
   "source": [
    "def load_data(train_path, val_path=None):\n",
    "  # Chargement du dataset d'entraînement\n",
    "  train_data = pd.read_csv(train_path)\n",
    "  print(f\"Diemnsion du dataset d'entraînement : {train_data.shape}\")\n",
    "  print(\"Aperçu du dataset d'entraînement : \")\n",
    "  print(train_data.head())\n",
    "\n",
    "  # Vérification des colonnes du dataset\n",
    "  print(\"\\nColonnes du dataset d'entraînement : \")\n",
    "  print(train_data.columns.tolist())\n",
    "\n",
    "  # si un fichier de validation\n",
    "  if val_path:\n",
    "    val_data = pd.read_csv(val_path)\n",
    "    print(f\"\\nDimensions du dataset de validation : {val_data.shape}\")\n",
    "    print(val_data.head())\n",
    "    return train_data, val_data\n",
    "  else:\n",
    "    return train_data, None\n",
    "  \n",
    "  # train_data = train_data.drop(columns=['topic'])\n",
    "  \n",
    "  # val_data = val_data.drop(columns=['topic'])\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eo99ZhvoG_pW"
   },
   "source": [
    "## 4. <a id=\"pretraitement\">Prétraitement du texte</a>\n",
    "Pourquoi le prétraitement est important ?\n",
    "1. **Réduction du bruit** : Les tweets contiennent souvent beucoup d'éléments non pertinents pour l'analyse des sentiments (URLs, mentions, etc.).\n",
    "2. **Normalisation** : Les différentes formes d'un même mot (pluriels, conjugaisons) sont ramenées à une forme santard.\n",
    "3. **Réduction de la dimensionnalité** : En supprimant les stopwords et en utilisant la lemmatisation, on réduit le nombre de mots uniques, ce qui facilite l'apprentissage des modèles.\n",
    "4. **Amélioration des performances** : Un texte bien prétraité permet aux modèles de se concentrer sur les mots et expressions qui véhiculent réeelement un sentiment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1741188674235,
     "user": {
      "displayName": "Arnaud Stadler",
      "userId": "13776147233644915596"
     },
     "user_tz": -60
    },
    "id": "yonVcc-7HMqc"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "  # Vérifier si le texte est une chaîne de caractères\n",
    "  if not isinstance(text, str):\n",
    "    return \"\"\n",
    "\n",
    "  # Convertir en minuscules\n",
    "  text = text.lower()\n",
    "\n",
    "  # Supprimer les URLs\n",
    "  text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "\n",
    "  # Supprimer les mentions utilisateurs (@user)\n",
    "  text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "  # Supprimer les hashtags\n",
    "  text = re.sub(r'#\\w+', '', text)\n",
    "\n",
    "  # Supprimer les caractères non-alphanumériques\n",
    "  text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "  # Supprimer les chiffres\n",
    "  text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "  # Tokenisation\n",
    "  tokens = word_tokenize(text)\n",
    "\n",
    "  # Supression des stopwords\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "  # Lemmatisation\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "  # Rejoindre les tokens\n",
    "  return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1741188676614,
     "user": {
      "displayName": "Arnaud Stadler",
      "userId": "13776147233644915596"
     },
     "user_tz": -60
    },
    "id": "gYEfDHNXJmYl"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(data, text_column, label_column):\n",
    "    \"\"\"\n",
    "    Prépare le dataset en excluant les tweets \"Irrelevant\" et en appliquant le prétraitement.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): Le DataFrame contenant les données\n",
    "        text_column (str): Le nom de la colonne contenant le texte des tweets\n",
    "        label_column (str): Le nom de la colonne contenant les labels de sentiment\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Le DataFrame prétraité\n",
    "    \"\"\"\n",
    "    # Vérifier que les colonnes existent\n",
    "    if text_column not in data.columns:\n",
    "        raise ValueError(f\"La colonne de texte '{text_column}' n'existe pas dans le dataset\")\n",
    "    if label_column not in data.columns:\n",
    "        raise ValueError(f\"La colonne d'étiquette '{label_column}' n'existe pas dans le dataset\")\n",
    "\n",
    "    # Filtrer les tweets \"Irrelevant\"\n",
    "    data = data[data[label_column] != 'Irrelevant'].copy()\n",
    "    print(f\"Nombre de tweets après filtrage des 'Irrelevant': {len(data)}\")\n",
    "\n",
    "    # Appliquer le prétraitement au texte\n",
    "    data['clean_text'] = data[text_column].apply(preprocess_text)\n",
    "\n",
    "    # Identifier les valeurs uniques dans la colonne des sentiments\n",
    "    unique_sentiments = data[label_column].unique()\n",
    "    print(f\"Valeurs uniques de sentiment trouvées : {unique_sentiments}\")\n",
    "\n",
    "    # Créer un mapping des sentiments basé sur les valeurs trouvées\n",
    "    sentiment_map = {}\n",
    "\n",
    "    # Essayer de détecter automatiquement le format du sentiment\n",
    "    if set(unique_sentiments).issubset({0, 1}) or set(unique_sentiments).issubset({'0', '1'}):\n",
    "        # Dataset binaire (positif/négatif)\n",
    "        sentiment_map = {0: 0, 1: 1, '0': 0, '1': 1}\n",
    "        print(\"Format détecté : Binaire (négatif/positif)\")\n",
    "    elif set(unique_sentiments).issubset({-1, 0, 1}) or set(unique_sentiments).issubset({'-1', '0', '1'}):\n",
    "        # Dataset ternaire avec -1, 0, 1\n",
    "        sentiment_map = {-1: 0, 0: 1, 1: 2, '-1': 0, '0': 1, '1': 2}\n",
    "        print(\"Format détecté : Ternaire (-1=négatif, 0=neutre, 1=positif)\")\n",
    "    elif any(isinstance(x, str) and x.lower() in ['positive', 'negative', 'neutral'] for x in unique_sentiments):\n",
    "        # Dataset avec texte\n",
    "        sentiment_map = {'Negative': 0, 'Neutral': 2, 'Positive': 1}\n",
    "        print(\"Format détecté : Textuel (negative/neutral/positive)\")\n",
    "    else:\n",
    "        # Format non reconnu, créer un mapping générique\n",
    "        sentiment_map = {val: idx for idx, val in enumerate(unique_sentiments)}\n",
    "        print(f\"Format non reconnu. Mapping créé : {sentiment_map}\")\n",
    "\n",
    "    # Appliquer le mapping\n",
    "    data['sentiment_label'] = data[label_column].map(sentiment_map)\n",
    "\n",
    "    # Vérifier qu'il n'y a pas de NaN dans les labels après le mapping\n",
    "    if data['sentiment_label'].isna().any():\n",
    "        print(\"ATTENTION : Certaines valeurs de sentiment n'ont pas pu être converties !\")\n",
    "        print(\"Valeurs problématiques : \", data[data['sentiment_label'].isna()][label_column].unique())\n",
    "        # Remplir les NaN avec une valeur par défaut (par exemple, 0 pour négatif)\n",
    "        data['sentiment_label'] = data['sentiment_label'].fillna(0)\n",
    "\n",
    "    return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmj2lXTYOpDL"
   },
   "source": [
    "## 5. <a id=\"visualisation\">Visualisation des données</a>\n",
    "\n",
    "- La partie visualisation des données est conçue pour explorer et comprendre les caractéristiques de dataset avant de passer à la modélisation.\n",
    "- La fonction `visualize_data()` crée trois visaulisations principales pour analyser la distribution et les caractéristiques des sentiments dans les tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1741188680367,
     "user": {
      "displayName": "Arnaud Stadler",
      "userId": "13776147233644915596"
     },
     "user_tz": -60
    },
    "id": "wRnMYAQOOlPS"
   },
   "outputs": [],
   "source": [
    "def visualize_data(data, sentiment_column, text_column):\n",
    "  \"\"\"Crée des visualisations pour explorer le dataset\"\"\"\n",
    "  # Distribution des sentiments\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  sentiment_counts = data[sentiment_column].value_counts()\n",
    "\n",
    "  #Création du palette de couleurs plus attrayante\n",
    "  colors = ['#ff9999', '#66b3ff', '#99ff99'][:len(sentiment_counts)]\n",
    "\n",
    "  # Affichage du graphique\n",
    "  ax = sentiment_counts.plot(kind='bar', color=colors)\n",
    "  for i, v in enumerate(sentiment_counts):\n",
    "    ax.text(i, v + 0.1, str(v), ha='center')\n",
    "\n",
    "  plt.title('Distribution des Sentiments', fontsize=14)\n",
    "  plt.xlabel('Sentiment', fontsize=12)\n",
    "  plt.ylabel('Nombre de Tweets', fontsize=12)\n",
    "  plt.tight_layout()\n",
    "  plt.savefig('sentiment_distribution.png')\n",
    "  print(\"Graphique de distributiondes sentiments sauvegardé dans 'sentiment_distribution.png\")\n",
    "\n",
    "  # Longueur des tweets par sentiment\n",
    "  data['text_length'] = data[text_column].astype(str).apply(len)\n",
    "\n",
    "  plt.figure(figsize=(12, 7))\n",
    "\n",
    "  # Utilisation de box plot avec swarmplot pour une meilleure visualisation\n",
    "  sample_size = int(len(data) * 0.05)\n",
    "  random_sample = data.sample(n=sample_size, random_state=42)\n",
    "  ax = sns.boxplot(x=sentiment_column, y='text_length', data=data, palette='Set2')\n",
    "  sns.swarmplot(x=sentiment_column, y='text_length', data=data, color='0.25', size=1, alpha=0.5)\n",
    "\n",
    "  plt.title('Longueur des Tweets par Sentiment', fontsize=14)\n",
    "  plt.xlabel('Sentiment', fontsize=12)\n",
    "  plt.ylabel('Longueur du Tweet (caractères)', fontsize=12)\n",
    "  plt.tight_layout()\n",
    "  plt.savefig('tweet_length_by_sentiment.png')\n",
    "  print(\"Graphique de longueur des tweets sauvegardé dans 'tweet_by_length_sentiment.png'\")\n",
    "\n",
    "  # Analyse des mots les plus fréquents par sentiment\n",
    "  from collections import Counter\n",
    "  import matplotlib.cm as cm\n",
    "\n",
    "  # Créer un DataFrame pour les mots les plus fréquents par sentiment\n",
    "  plt.figure(figsize=(15, 12))\n",
    "\n",
    "  # Définir le nombre de sentiments dans le dataset\n",
    "  num_sentiments = data['sentiment_label'].nunique()\n",
    "\n",
    "  # Ajuster le nombre de sous-graphique en fonction du nombre de sentiment\n",
    "  fig, axes = plt.subplots(1, num_sentiments, figsize=(15, 6))\n",
    "  if num_sentiments == 1:\n",
    "    axes = [axes]     # Assure que axes est toujours une liste\n",
    "\n",
    "  sentiment_names = {0: 'Négatif', 1: 'Positif', 2: 'Neutre'}\n",
    "\n",
    "  # Pour chaque sentiment, trouver les mots les plus fréquents\n",
    "  for i, sentiment_value in enumerate(sorted(data['sentiment_label'].unique())):\n",
    "    # Filtrer les tweets par sentiment\n",
    "    sentiment_data = data[data['sentiment_label'] == sentiment_value]\n",
    "\n",
    "    # Joindre tous les textes nettoyés\n",
    "    all_words = ' '.join(sentiment_data['clean_text'].astype(str)).split()\n",
    "\n",
    "    # Compter les mots\n",
    "    word_counts = Counter(all_words)\n",
    "\n",
    "    # Prendre les 15 mots les plus fréquents\n",
    "    most_common = word_counts.most_common(15)\n",
    "\n",
    "    # Créer des listes pour les graphiques\n",
    "    words = [word for word, count in most_common]\n",
    "    counts = [count for word, count in most_common]\n",
    "\n",
    "    # Tracer le graphique à barre horizontales\n",
    "    sentiment_name = sentiment_names.get(sentiment_value, f\"Sentiment {sentiment_value}\")\n",
    "    axes[i].barh(words, counts, color=cm.Set3(i / num_sentiments))\n",
    "    axes[i].set_title(f'Mots fréquent - {sentiment_name}')\n",
    "    axes[i].set_xlabel('Fréquence')\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.savefig('frequent_words_by_sentiment.png')\n",
    "  print(\"Graphique des mots fréquents sauvegardé dans 'frequent_words_by_sentiment.png'\")\n",
    "\n",
    "  return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSvk1Q7-b0eO"
   },
   "source": [
    "Ces visulisations constituent une étape d'analyse exploratoire des données (EDA) importante qui aide à :\n",
    "- Comprendre le déséquilibre éventuel des classes.\n",
    "- Voir si la longueur des tweets est corrélée au sentiment.\n",
    "- Identifier les mots caractéristiques de chaque sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xG0BjmCeZDZ5"
   },
   "source": [
    "## 6. <a id=\"tradi-tdidf\">Approche Traditionnelle avec TF-IDF</a>\n",
    "\n",
    "Cette section implémente une méthode classqieu d'analyse de sentiments basé sur des techniques de NLP plus traditionnelles, avant l'ère des transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1741188684017,
     "user": {
      "displayName": "Arnaud Stadler",
      "userId": "13776147233644915596"
     },
     "user_tz": -60
    },
    "id": "xZDHZjp4ZBrr"
   },
   "outputs": [],
   "source": [
    "def train_tfidf_model(train_data, val_data=None):\n",
    "  # Si pas de données de validation, utiliser une partie des données d'entraînement\n",
    "  if val_data is None:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        train_data['clean_text'], train_data['sentiment_label'],\n",
    "        test_size=0.2, random_state=42\n",
    "    )\n",
    "  else:\n",
    "    X_train = train_data['clean_text']\n",
    "    y_train = train_data['sentiment_label']\n",
    "    X_val = val_data['clean_text']\n",
    "    y_val = val_data['sentiment_label']\n",
    "\n",
    "  print(f\"Taille des données d'entraînement : {len(X_train)}\")\n",
    "  print(f\"Taille des données de validation : {len(X_val)}\")\n",
    "\n",
    "  # Vectorisation TF-IDF\n",
    "  print(\"Vectorisation TF-IDF des données ...\")\n",
    "  tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "  X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "  X_val_tdidf = tfidf_vectorizer.transform(X_val)\n",
    "\n",
    "  # Entraînement du modèle (Régression Logistique)\n",
    "  print(\"Entraînement du modèle de régression logistiques ...\")\n",
    "  lr_model = LogisticRegression(max_iter=1000, verbose=1, n_jobs=1, C=0.1, penalty='l2')\n",
    "  lr_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "  # Évaluation du modèle\n",
    "  y_pred = lr_model.predict(X_val_tdidf)\n",
    "  accuracy = accuracy_score(y_val, y_pred)\n",
    "  print(f\"Précision du modèle TF-IDF + Régression Logistique : {accuracy:.4f}\")\n",
    "\n",
    "  # Rapport de classification détaillé\n",
    "  print(\"\\nRapport de classification:\")\n",
    "  print(classification_report(y_val, y_pred))\n",
    "\n",
    "  # Matrice de confusion\n",
    "  cm = confusion_matrix(y_val, y_pred)\n",
    "  plt.figure(figsize=(10, 8))\n",
    "\n",
    "  # Utiliser un heatmap plus informatif\n",
    "  sns.heatmap(\n",
    "      cm,\n",
    "      annot=True,\n",
    "      fmt='d',\n",
    "      cmap='Blues',\n",
    "      xticklabels=['Négatif', 'Positif', 'Neutre'][:lr_model.classes_.size],\n",
    "      yticklabels=['Négatif', 'Positif', 'Neutre'][:lr_model.classes_.size]\n",
    "  )\n",
    "  plt.title('Matrice de confusion - Modèle TF-IDF', fontsize=14)\n",
    "  plt.xlabel('Prédiction', fontsize=12)\n",
    "  plt.ylabel('Réalité', fontsize=12)\n",
    "  plt.tight_layout()\n",
    "  plt.savefig('confusion_matrix_tfidf.png')\n",
    "  print(\"Matrice de confusion suavegardée dans 'confusion_matrix_tfidf.png'\")\n",
    "\n",
    "  # Analye des caractéristiques les plus importantes\n",
    "  if hasattr(lr_model, 'coef_'):\n",
    "    for i, class_label in enumerate(lr_model.classes_):\n",
    "      # Obtenir les coefficients pour cette classe\n",
    "      coefficients = lr_model.coef_[i]\n",
    "\n",
    "      # Obtenir les noms des caractéristiques\n",
    "      feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "      # Créer un DataFrame pour faciliter le tri\n",
    "      coefficients_df = pd.DataFrame({\n",
    "          'feature': feature_names,\n",
    "          'coefficient': coefficients\n",
    "      })\n",
    "\n",
    "      # Trier par coefficient absolu décroissant\n",
    "      sorted_coefficients = coefficients_df.reindex(\n",
    "          coefficients_df['coefficient'].abs().sort_values(ascending=False).index\n",
    "      )\n",
    "\n",
    "      # Afficher les 10 caractéristiques les plus importantes pour cette classe\n",
    "      sentiment_name = ['Négatif', 'Positif', 'Neutre'][i] if i < 3 else f\"Classe {i}\"\n",
    "      print(f\"\\n10 mots les plus importants pour la classe {sentiment_name} : \")\n",
    "      print(sorted_coefficients.head(10))\n",
    "\n",
    "  return lr_model, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. <a id=\"deep-learning\">Approche Deep Learning avec Transformers</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === APPROCHE DEEP LEARNING AVEC TRANSFORMERS ===\n",
    "def train_transformer_model(train_data, val_data=None, model_name=\"distilbert-base-uncased\"):\n",
    "  \"\"\"Entraîne un modèle BERT pour l'analyse des sentiments\"\"\"\n",
    "  print(f\"Chargement du modèle pré-entraîné: {model_name}\")\n",
    "  \n",
    "  # Déterminer le nombre de classes\n",
    "  num_labels = train_data['sentiment_label'].nunique()\n",
    "  print(f\"Nombre de classes détecté: {num_labels}\")\n",
    "  \n",
    "  # Chargement du modèle pré-entraîné\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=num_labels\n",
    "  )\n",
    "  \n",
    "  # Si pas de données de validation, utiliser une partie des données d'entraînement\n",
    "  if val_data is None:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        train_data['clean_text'].tolist(), train_data['sentiment_label'].tolist(), \n",
    "        test_size=0.2, random_state=42\n",
    "    )\n",
    "  else:\n",
    "    train_texts = train_data['clean_text'].tolist()\n",
    "    train_labels = train_data['sentiment_label'].tolist()\n",
    "    val_texts = val_data['clean_text'].tolist()\n",
    "    val_labels = val_data['sentiment_label'].tolist()\n",
    "\n",
    "  print(f\"Taille des données d'entraînement: {len(train_texts)}\")\n",
    "  print(f\"Taille des données de validation: {len(val_texts)}\")\n",
    "\n",
    "  # Fonction de tokenisation pour BERT\n",
    "  def tokenize_function(examples):\n",
    "    return tokenizer(examples, padding=\"max_length\", truncation=True, max_length=128)\n",
    "  \n",
    "  print(\"Tokenisation des données...\")\n",
    "  # Création des encodages\n",
    "  train_encodings = tokenize_function(train_texts)\n",
    "  val_encodings = tokenize_function(val_texts)\n",
    "  \n",
    "  # Création d'une classe de dataset compatible avec Hugging Face\n",
    "  class TwitterDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "      self.encodings = encodings\n",
    "      self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "      item['labels'] = torch.tensor(self.labels[idx])\n",
    "      return item\n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.labels)\n",
    "\n",
    "  train_dataset = TwitterDataset(train_encodings, train_labels)\n",
    "  val_dataset = TwitterDataset(val_encodings, val_labels)\n",
    "\n",
    "  # Configuration de l'entraînement\n",
    "  training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",  # Désactiver les rapports pour simplifier\n",
    "  )\n",
    "  \n",
    "  # Création du Trainer\n",
    "  trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    "  )\n",
    "  \n",
    "  # Entraînement du modèle\n",
    "  print(\"Début de l'entraînement du modèle Transformer...\")\n",
    "  trainer.train()\n",
    "  \n",
    "  # Évaluation\n",
    "  print(\"Évaluation du modèle...\")\n",
    "  eval_results = trainer.evaluate()\n",
    "  print(f\"Résultats de l'évaluation: {eval_results}\")\n",
    "\n",
    "  # Sauvegarde du modèle et du tokenizer\n",
    "  print(\"Sauvegarde du modèle et du tokenizer...\")\n",
    "  model.save_pretrained(\"./twitter_sentiment_model\")\n",
    "  tokenizer.save_pretrained(\"./twitter_sentiment_tokenizer\")\n",
    "  \n",
    "  return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7WZ2bpgxnPt"
   },
   "source": [
    "## <a id=\"fonction-principale\">Fonction Principale</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E3Mjgar1xl7I",
    "outputId": "e17fa3a5-6ca4-46dd-8923-709d4f60d494"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "  print(\"=== Analyse des sentiments sur TWITTER ===\")\n",
    "  print(\"Début du traitement...\\n\")\n",
    "\n",
    "  # Chemins vers les datasets\n",
    "  train_path = \"data/twitter_training.csv\"\n",
    "  val_path = \"data/twitter_validation.csv\"\n",
    "\n",
    "  # Noms des colonnes\n",
    "  text_column = \"text\"\n",
    "  label_column = \"sentiment\"\n",
    "\n",
    "  # Chargement des données\n",
    "  print(\"Chargement des données...\")\n",
    "  train_data, val_data = load_data(train_path, val_path)\n",
    "\n",
    "  # Prétraitement\n",
    "  print(\"\\nPrétraitement des données...\")\n",
    "  processed_train = prepare_dataset(train_data, text_column, label_column)\n",
    "\n",
    "  if val_data is not None:\n",
    "    processed_val = prepare_dataset(val_data, text_column, label_column)\n",
    "  else:\n",
    "    processed_val = None\n",
    "\n",
    "  # Visualisation\n",
    "  print(\"\\nCréation des visualisation...\")\n",
    "  visualize_data(processed_train, label_column, text_column)\n",
    "\n",
    "  # Demander à l'utilisateur quelle approche utiliser\n",
    "  print(\"\\nChoisissez votre approche d'analyse:\")\n",
    "  print(\"1. Approche traditionnelle (TF-IDF + Régression Logistique)\")\n",
    "  print(\"2. Approche Deep Learning (Transformers)\")\n",
    "  print(\"3. Les deux approches\")\n",
    "\n",
    "  choice = input(\"Votre choix (1, 2 ou 3): \")\n",
    "\n",
    "  if choice in [\"1\", \"3\"]:\n",
    "    print(\"\\n=== APPROCHE TRADITIONNELLE ===\")\n",
    "    lr_model, tfidf_vectorizer = train_tfidf_model(processed_train, processed_val)\n",
    "\n",
    "  if choice in [\"2\", \"3\"]:\n",
    "    print(\"\\n=== APPROCHE DEEP LEARNING ===\")\n",
    "    transformer_model, tokenizer = train_transformer_model(processed_train, processed_val)\n",
    "\n",
    "  print(\"\\n===TRAITEMENT TERMINÉ ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
